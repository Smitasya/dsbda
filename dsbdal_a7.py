# -*- coding: utf-8 -*-
"""Akanksha_DSBDAL_A7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SghgqGyCklEifpEELyn93EQYDxd5zaiy

# **Text Analysis**

**Extract Sample document and apply following document preprocessing methods:
Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization**

**Install package**
"""

pip install nltk

"""**Import libraries**"""

import nltk
import re

"""**Download the required packages**"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""**Initialize the text**"""

text= "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."

"""**Perform Tokenization**

Sentence Tokenization
"""

from nltk.tokenize import sent_tokenize
tokenized_text= sent_tokenize(text)
print(tokenized_text)

"""Word Tokenization"""

from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)

"""**Removing Punctuations and Stop Word**"""

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)
text= "How to remove stop words with NLTK library in Python?"
text= re.sub('[^a-zA-Z]', ' ',text)
tokens = word_tokenize(text.lower())
filtered_text=[]
for w in tokens:
  if w not in stop_words:
    filtered_text.append(w)
print("Tokenized Sentence:",tokens)
print("Filterd Sentence:",filtered_text)

"""**Perform Stemming**"""

from nltk.stem import PorterStemmer
e_words= ["wait", "waiting", "waited", "waits"]
ps =PorterStemmer()
for w in e_words:
  rootWord=ps.stem(w)
print(rootWord)

"""**Perform Lemmatization**"""

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print("Lemma for {} is {}".format(w,
wordnet_lemmatizer.lemmatize(w)))

"""**Apply POS Tagging to text**"""

import nltk
from nltk.tokenize import word_tokenize
data="The pink sweater fit her perfectly"
words=word_tokenize(data)
for word in words:
  print(nltk.pos_tag([word]))

"""***Algorithm for Create representation of document by calculating TFIDF***

**Step 1: Import the necessary libraries.**
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

"""**Step 2: Initialize the Documents.**"""

documentA = 'Jupiter is the largest Planet'
documentB = 'Mars is the fourth planet from the Sun'

"""**Step 3: Create BagofWords (BoW) for Document A and B.**"""

bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')

"""**Step 4: Create Collection of Unique words from Document A and B.**"""

uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))

"""**Step 5: Create a dictionary of words and their occurrence for each document in the
corpus**
"""

numOfWordsA = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsA:
  numOfWordsA[word] += 1
  numOfWordsB = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsB:
  numOfWordsB[word] += 1

"""**Step 6: Compute the term frequency for each of our documents.**"""



def computeTF(wordDict, bagOfWords):
tfDict = {}
bagOfWordsCount = len(bagOfWords)
for word, count in wordDict.items():
tfDict[word] = count / float(bagOfWordsCount)
return tfDict
tfA = computeTF(numOfWordsA, bagOfWordsA)
tfB = computeTF(numOfWordsB, bagOfWordsB)

"""**Step 7: Compute the term Inverse Document Frequency.**"""

def computeIDF(documents):
  import math
  N = len(documents)
  idfDict = dict.fromkeys(documents[0].keys(), 0)
  for document in documents:
    for word, val in document.items():
      if val > 0:
        idfDict[word] += 1
  for word, val in idfDict.items():
    idfDict[word] = math.log(N / float(val))
  return idfDict
  idfs = computeIDF([numOfWordsA, numOfWordsB])
  idfs

"""**Step 8: Compute the term TF/IDF for all words.a**"""

def computeTFIDF(tfBagOfWords, idfs):
  tfidf = {}
  for word, val in tfBagOfWords.items():
    tfidf[word] = val * idfs[word]
  return tfidf
  tfidfA = computeTFIDF(tfA, idfs)
  tfidfB = computeTFIDF(tfB, idfs)
  df = pd.DataFrame([tfidfA, tfidfB])
  df